{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"fig/iacs.png\"> S-109A Introduction to Data Science \n",
    "## Homework 4 - Regularization \n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Summer 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "Names of people you have worked with goes here: \n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling(): styles = open(\"cs109.css\", \"r\").read(); return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "\n",
    "from pandas.core import datetools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Bike Sharing Usage Data\n",
    "\n",
    "In this homework, we will focus on regularization and cross validation. We will continue to build regression models for the Capital Bikeshare program in Washington D.C.  See homework 3 for more information about the Capital Bikeshare data that we'll be using extensively. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "\n",
    "\n",
    "<div class='exercise'> <b> Question 1 </b> </div> \n",
    "  In HW3 Questions 1-3, you preprocessed the data in preparation for your regression analysis. We ask you to repeat those steps (particularly those in Question 3) so that we can compare the analysis models in this HW with those you developed in HW3.  In this HW we'll be using models from sklearn exclusively (as opposed to statsmodels)\n",
    "  \n",
    "**1.1** [From HW3] Read `data/BSS_train.csv` and `data/BSS_test.csv` into dataframes `BSS_train` and `BSS_test`, respectively.  Remove the `dteday` column from both train and test dataset. We do not need it, and its format cannot be used for analysis.  Also remove the `casual` and `registered` columns for both training and test datasets as they make  `count` trivial.   \n",
    "\n",
    "**1.2** Since we'll be exploring Regularization and Polynomial Features, it will make sense to standardize our data.  Standardize the numerical features. Store the dataframes for the processed training and test predictors into the variables `X_train` and `X_test`.  Store the appropriately shaped numpy arrays for the corresponding train and test `count` columns into `y_train` and `y_test`.\n",
    "\n",
    "**1.3** Use the `LinearRegression` library from `sklearn` to fit a multiple linear regression model to the training set data in `X_train`.  Store the fitted model in the variable `BikeOLSModel`.\n",
    "\n",
    "**1.4** What are the training and test set $R^2$ scores?  Store the training and test $R^2$ scores of the `BikeOLSModel` in a dictionary `BikeOLS_r2scores` using the string 'training' and 'test' as keys.  \n",
    "\n",
    "**1.5**   We're going to use bootstrapped confidence intervals (use 500 bootstrap iterations) to determine which of the estimated coefficients for the `BikeOLSModel` are statistically significant at a significance level of 5% .  We'll do so by creating 3 different functions:\n",
    "\n",
    "1. `make_bootstrap_sample(dataset_X, dataset_y)` returns a bootstrap sample of `dataset_X` and `dataset_y`\n",
    "2. `calculate_coefficients(dataset_X, dataset_y, model)` returns in the form of a dictionary regression coefficients calculated by your model on `dataset_X` and `dataset_y`.  The keys for regression coefficients dictionary should be the names of the features.  The values should be the coefficient values of that feature calculated on your model.  An example would be {'hum': 12.3, 'windspeed': -1.2, 'Sunday': 0.6 ... }\n",
    "3. `get_significant_predictors(regression_coefficients, significance_level)` takes as input a list of regression coefficient dictionaries (each one the output of `calculate_coefficients` and  returns a python list of the feature names of the significant predictors e.g. ['Monday', 'hum', 'holiday', ... ]\n",
    "\n",
    "In the above functions `dataset_X` should always be a pandas dataframe with your features, `dataset_y` a numpy column vector with the values of the response variable and collectively they form the dataset upon which the operations take place. `model` is the `sklearn` regression model that will be used to generate the regression coefficients. `regression_coefficients` is a list of dictionaries of numpy arrays with each numpy array containing the regression coefficients (not including the intercept) calculated from one bootstrap sample.  `significance_level` represents the significance level as a floating point number.  So a 5% significance level should be represented as 0.05.  \n",
    "\n",
    "\n",
    "Store the feature names as a list of strings in the variable `BikeOLS_significant_bootstrap` and print them for your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Read `data/BSS_train.csv` and `data/BSS_test.csv` into Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>holiday</th>\n",
       "      <th>year</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>counts</th>\n",
       "      <th>spring</th>\n",
       "      <th>...</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Mon</th>\n",
       "      <th>Tue</th>\n",
       "      <th>Wed</th>\n",
       "      <th>Thu</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Sat</th>\n",
       "      <th>Cloudy</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Storm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.00000</td>\n",
       "      <td>13903.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.529454</td>\n",
       "      <td>0.029274</td>\n",
       "      <td>0.504567</td>\n",
       "      <td>0.680644</td>\n",
       "      <td>0.496732</td>\n",
       "      <td>0.475426</td>\n",
       "      <td>0.629051</td>\n",
       "      <td>0.190025</td>\n",
       "      <td>187.885492</td>\n",
       "      <td>0.253183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085305</td>\n",
       "      <td>0.140977</td>\n",
       "      <td>0.141121</td>\n",
       "      <td>0.144861</td>\n",
       "      <td>0.140833</td>\n",
       "      <td>0.142128</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.260375</td>\n",
       "      <td>0.08286</td>\n",
       "      <td>0.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.917884</td>\n",
       "      <td>0.168580</td>\n",
       "      <td>0.499997</td>\n",
       "      <td>0.466244</td>\n",
       "      <td>0.192699</td>\n",
       "      <td>0.171951</td>\n",
       "      <td>0.193100</td>\n",
       "      <td>0.122009</td>\n",
       "      <td>180.113476</td>\n",
       "      <td>0.434850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279346</td>\n",
       "      <td>0.348010</td>\n",
       "      <td>0.348158</td>\n",
       "      <td>0.351973</td>\n",
       "      <td>0.347862</td>\n",
       "      <td>0.349194</td>\n",
       "      <td>0.352625</td>\n",
       "      <td>0.438855</td>\n",
       "      <td>0.27568</td>\n",
       "      <td>0.011993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>977.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               hour       holiday          year    workingday          temp  \\\n",
       "count  13903.000000  13903.000000  13903.000000  13903.000000  13903.000000   \n",
       "mean      11.529454      0.029274      0.504567      0.680644      0.496732   \n",
       "std        6.917884      0.168580      0.499997      0.466244      0.192699   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.020000   \n",
       "25%        6.000000      0.000000      0.000000      0.000000      0.340000   \n",
       "50%       11.000000      0.000000      1.000000      1.000000      0.500000   \n",
       "75%       18.000000      0.000000      1.000000      1.000000      0.660000   \n",
       "max       23.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              atemp           hum     windspeed        counts        spring  \\\n",
       "count  13903.000000  13903.000000  13903.000000  13903.000000  13903.000000   \n",
       "mean       0.475426      0.629051      0.190025    187.885492      0.253183   \n",
       "std        0.171951      0.193100      0.122009    180.113476      0.434850   \n",
       "min        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "25%        0.333300      0.480000      0.104500     39.000000      0.000000   \n",
       "50%        0.484800      0.630000      0.194000    141.000000      0.000000   \n",
       "75%        0.621200      0.780000      0.253700    280.000000      1.000000   \n",
       "max        1.000000      1.000000      0.850700    977.000000      1.000000   \n",
       "\n",
       "           ...                Dec           Mon           Tue           Wed  \\\n",
       "count      ...       13903.000000  13903.000000  13903.000000  13903.000000   \n",
       "mean       ...           0.085305      0.140977      0.141121      0.144861   \n",
       "std        ...           0.279346      0.348010      0.348158      0.351973   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                Thu           Fri           Sat        Cloudy         Snow  \\\n",
       "count  13903.000000  13903.000000  13903.000000  13903.000000  13903.00000   \n",
       "mean       0.140833      0.142128      0.145508      0.260375      0.08286   \n",
       "std        0.347862      0.349194      0.352625      0.438855      0.27568   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "75%        0.000000      0.000000      0.000000      1.000000      0.00000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.00000   \n",
       "\n",
       "              Storm  \n",
       "count  13903.000000  \n",
       "mean       0.000144  \n",
       "std        0.011993  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "BSS_train = pd.read_csv('data/BSS_train.csv')\n",
    "BSS_test = pd.read_csv('data/BSS_test.csv')\n",
    "\n",
    "BSS_train = BSS_train.drop(['dteday','casual','registered', 'Unnamed: 0'], axis=1)\n",
    "BSS_test = BSS_test.drop(['dteday','casual','registered', 'Unnamed: 0'], axis=1)\n",
    "\n",
    "BSS_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Standardizing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Generating training set including standardizing\n",
    "X_train_orig = BSS_train.loc[:,BSS_train.columns != 'counts']\n",
    "X_test_orig = BSS_test.loc[:,BSS_train.columns != 'counts']\n",
    "\n",
    "\n",
    "# scaling only the continuous predictors\n",
    "categorical_columns = ['hour', 'holiday', 'year', 'workingday', 'spring', 'summer',\n",
    "                       'fall', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sept', \n",
    "                       'Oct', 'Nov', 'Dec', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat',\n",
    "                       'Cloudy', 'Snow', 'Storm']\n",
    "numerical_columns = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "\n",
    "X_train_numerical = X_train_orig[numerical_columns]\n",
    "X_test_numerical = X_test_orig[numerical_columns]\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_numerical)\n",
    "\n",
    "# create and scale the training/test set\n",
    "X_train = X_train_orig.copy()\n",
    "X_test = X_test_orig.copy()\n",
    "\n",
    "X_train[numerical_columns] = scaler.transform(X_train_numerical)\n",
    "X_test[numerical_columns] = scaler.transform(X_test_numerical)\n",
    "\n",
    "y_train = BSS_train['counts'].values.reshape(-1,1)\n",
    "y_test = BSS_test['counts'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Use the `LinearRegression` library from `sklearn` to fit a multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "BikeOLSModel = LinearRegression()\n",
    "BikeOLSModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 What are the training and test set $R^2$ scores? Store the $R^2$ scores of the `BikeOLSModel` on the training and test sets in a dictionary `BikeOLS_r2scores`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set R^2 Score: 0.4065\n",
      "Test Set R^2 Score: 0.4064\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "train_r2_score = r2_score(y_train, BikeOLSModel.predict(X_train))\n",
    "test_r2_score = r2_score(y_test, BikeOLSModel.predict(X_test))\n",
    "print(\"Train Set R^2 Score: {:.4f}\".format(train_r2_score))\n",
    "print(\"Test Set R^2 Score: {:.4f}\".format(test_r2_score))\n",
    "\n",
    "BikeOLS_r2scores = {}\n",
    "BikeOLS_r2scores['training'] = train_r2_score\n",
    "BikeOLS_r2scores['test'] = test_r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 We're going to use bootstrapped confidence intervals to determine which of the estimated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# dataset_x should be a pandas dataframe\n",
    "\n",
    "## accepts dataset inputs as numpy arrays\n",
    "def make_bootstrap_sample(dataset_X, dataset_y, size = None):\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    # by default return a bootstrap sample of the same size as the original dataset\n",
    "    if not size: size = len(dataset_X)\n",
    "    \n",
    "    # if the X and y datasets aren't the same size, raise an exception\n",
    "    if len(dataset_X) != len(dataset_y):\n",
    "        raise Exception(\"Data size must match between dataset_X and dataset_y\")\n",
    "    \n",
    "    \n",
    "    sample_indices = np.random.choice(size, size=size, replace=True)\n",
    "    \n",
    "    bootstrap_dataset_X = dataset_X.iloc[sample_indices, :]\n",
    "    bootstrap_dataset_y = dataset_y[sample_indices]\n",
    "    \n",
    "    # return as a tuple your bootstrap samples of dataset_X as a pandas dataframe\n",
    "    # and your bootstrap samples of dataset y as a numpy column vector\n",
    "    \n",
    "    return (bootstrap_dataset_X, bootstrap_dataset_y)\n",
    "    \n",
    "\n",
    "def calculate_coefficients(dataset_X, dataset_y, model):\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(dataset_X, dataset_y) \n",
    "    \n",
    "    coefficients_dictionary = {k:v for k, v in zip(dataset_X.columns, model.coef_.ravel())}\n",
    "    \n",
    "    # return coefficients  in the variable  coefficients_dictioanry as a dictionary\n",
    "    # with the key being the name of the feature as a string\n",
    "    # the value being the value of the coefficients\n",
    "    # do not return the intercept as part of this\n",
    "    return coefficients_dictionary\n",
    "\n",
    "\n",
    "def get_significant_predictors(regression_coefficients, significance_level):\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    # regression_coefficients is a list of dictionaries\n",
    "    # with the key being the name of the feature as a string\n",
    "    # the value being the value of the coefficients\n",
    "    # each dictionary in th list should be the output of calculate_coefficients\n",
    "    \n",
    "    if (len(regression_coefficients) <= 0):\n",
    "        return []\n",
    "    \n",
    "    coeff_names = np.array(list(regression_coefficients[0].keys()), dtype='object')\n",
    "    coeff_samples = [list(coeff_list.values()) for coeff_list in regression_coefficients]\n",
    "    \n",
    "    coeff_samples = np.array(coeff_samples)\n",
    "    \n",
    "    # Obtain bottom percentile values\n",
    "    bottom_percentile = np.percentile(coeff_samples, q=significance_level/2, axis=0)\n",
    "    \n",
    "    # Obtain top percentile values\n",
    "    top_percentile = np.percentile(coeff_samples, q=1-significance_level/2, axis=0)\n",
    "    \n",
    "    # Coefficients with bottom value greater than 0 or top value less than 0 are signficant\n",
    "    significant_index = ((bottom_percentile > 0.0) | (top_percentile < 0.0))\n",
    " \n",
    "    significant_coefficients = list(coeff_names[significant_index])\n",
    "      \n",
    "    # return the significant coefficients as a list of strings\n",
    "    return significant_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of Bootstrap samplse to 500\n",
    "N_bootstrap_samples = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hour',\n",
       " 'holiday',\n",
       " 'year',\n",
       " 'workingday',\n",
       " 'temp',\n",
       " 'atemp',\n",
       " 'hum',\n",
       " 'spring',\n",
       " 'summer',\n",
       " 'fall',\n",
       " 'Feb',\n",
       " 'Mar',\n",
       " 'Apr',\n",
       " 'May',\n",
       " 'Jun',\n",
       " 'Jul',\n",
       " 'Aug',\n",
       " 'Sept',\n",
       " 'Oct',\n",
       " 'Nov',\n",
       " 'Dec',\n",
       " 'Mon',\n",
       " 'Tue',\n",
       " 'Wed',\n",
       " 'Thu',\n",
       " 'Fri',\n",
       " 'Sat',\n",
       " 'Snow']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_coefficients = []\n",
    "for i in range(N_bootstrap_samples):\n",
    "    sample = make_bootstrap_sample(X_train, y_train)\n",
    "    coefficient_dict = calculate_coefficients(sample[0], sample[1], LinearRegression())\n",
    "    regression_coefficients.append(coefficient_dict)\n",
    "    \n",
    "get_significant_predictors(regression_coefficients, 0.05)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalization Methods\n",
    "\n",
    "In HW 3 Question 5 we explored using subset selection to find a significant subset of features.  We then fit a regression model just on that subset of features instead of on the full dataset (including all features).   As an alternative to selecting a subset of predictors and fitting a regression model on the subset, one can fit a linear regression model on all predictors, but shrink or regularize the coefficient estimates to make sure that the model does not \"overfit\" the training set. \n",
    "\n",
    "<div class='exercise'> <b> Question 2 </b> </div> \n",
    "  We're going to use Ridge and Lasso regression regularization techniques to fit linear models to the training set.  We'll use cross-validation and shrinkage parameters $\\lambda$ from the set $\\{.001,.005,1,5,10,50,100,500,1000\\}$ to pick the best model for each regularization technique.\n",
    "\n",
    "**2.1** Use 5-fold cross-validation to pick the best shrinkage parameter from the set $\\{.001,.005,1,5,10,50,100,500,1000\\}$ for your Ridge Regression model on the training data.  Fit a Ridge Regression model on the training set with the selected shrinkage parameter and store your fitted model in the variable `BikeRRModel`.  Store the selected shrinkage parameter in the variable `BikeRR_shrinkage_parameter`.\n",
    "\n",
    "**2.2** Use 5-fold cross-validation to pick the best shrinkage parameter from the set $\\{.001,.005,1,5,10,50,100,500,1000\\}$ for your Lasso Regression model on the training data.  Fit a Lasso Regression model on the training set with the selected shrinkage parameter and store your fitted model in the variable `BikeLRModel`.  Store the selected shrinkage parameter in the variable `BikeLR_shrinkage_parameter`.\n",
    "\n",
    "**2.3** Create three dictionaries `BikeOLSparams`, `BikeLRparams`, and `BikeRRparams`.  Store in each the corresponding regression coefficients for each of the regression models indexed by the string feature name.\n",
    "\n",
    "**2.4** For the Lasso and Ridge Regression models list the features that are assigned a coefficient value close to 0 (i.e. the absolute value of the coefficient is less than 0.1).  How closely do they match the redundant predictors found (if any) in HW 3, Question 5?\n",
    "\n",
    "**2.5** To get a visual sense of how the features different regression models (Multiple Linear Regression, Ridge Regression, Lasso Regression) estimate coefficients, order the features by magnitude of the estimated coefficients in the Multiple Linear Regression Model (no shrinkage).  Plot a bar graph of the magnitude (absolute value) of the estimated coefficients from Multiple Linear Regression in order from greatest to least.  Using a different color (and alpha values) overlay bar graphs of the magnitude of the estimated coefficients (in the same order as the Multiple Linear Regression coefficients) from Ridge and Lasso Regression.\n",
    "\n",
    "**2.6** Let's examine a pair of features we believe to be related.  Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors `temp` and `atemp`?\n",
    "\n",
    "**2.7** Discuss the Results:\n",
    "\n",
    "1. How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrinkage penalty) in Question 1? \n",
    "2. Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference.\n",
    "3. Is the significance related to the shrinkage in some way?\n",
    "\n",
    "*Hint:* You may use `sklearn`'s `RidgeCV` and `LassoCV` classes to implement Ridge and Lasso regression. These classes automatically perform cross-validation to tune the parameter $\\lambda$ from a given range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [.001, .005, 1, 5, 10, 50, 100, 500, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Use 5-fold cross-validation to pick the best shrinkage parameter from the set $\\{.001,.005,1,5,10,50,100,500,1000\\}$ for your Ridge Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# RidgeCV\n",
    "BikeRRModel = RidgeCV(cv=5, alphas=lambdas, fit_intercept=True)\n",
    "\n",
    "BikeRRModel.fit(X_train, y_train) \n",
    "\n",
    "BikeRR_shrinkage_parameter = BikeRRModel.alpha_\n",
    "\n",
    "Ridge_coefficients = BikeRRModel.coef_.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Use 5-fold cross-validation to pick the best shrinkage parameter from the set $\\{.001,.005,1,5,10,50,100,500,1000\\}$ for your Lasso Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Lasso\n",
    "BikeLRModel = LassoCV(cv=5, alphas=lambdas, fit_intercept=True)\n",
    "\n",
    "BikeLRModel.fit(X_train, y_train)\n",
    "\n",
    "BikeLR_shrinkage_parameter = BikeLRModel.alpha_\n",
    "Lasso_coefficients = BikeLRModel.coef_.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Create three dictionaries `BikeOLSparams`, `BikeLRparams`, and `BikeRRparams`.  Store in each the corresponding regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "BikOLSparams = {k: v for k,v in zip(feature_names, BikeOLSModel.coef_.ravel())}\n",
    "BikeRRparams = {k: v for k,v in zip(feature_names, BikeRRModel.coef_.ravel())}\n",
    "BikeLRparams = {k: v for k,v in zip(feature_names, BikeLRModel.coef_.ravel())}   # coef_ of LassoCV is 1-D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 For the Lasso and Ridge Regression models list the features that are assigned a coefficient value close to 0.  How closely do they match the redundant predictors found (if any) in HW 3, Question 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge predictors within 0.1 of 0: []\n",
      "----\n",
      "Lasso predictors within 0.1 of 0: ['Mon']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "print(\"Ridge predictors within 0.1 of 0:\", list(feature_names[np.abs(Ridge_coefficients)<.1]))\n",
    "print(\"----\")\n",
    "print(\"Lasso predictors within 0.1 of 0:\", list(feature_names[np.abs(Lasso_coefficients)<.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "There is no resemblance between the list of insignficant coefficients and the redundant predictors found in HW3, Q5.\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 To get a visual sense of how the features different regression models (Multiple Linear Regression, Ridge Regression, Lasso Regression) estimate coefficients, order the features by magnitude of the estimated coefficients in the Multiple Linear Regression Model (no shrinkage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "si= np.argsort(np.abs(BikeOLSModel.coef_[0]))\n",
    "\n",
    "n_coeffs = len(feature_names)\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.bar(np.arange(n_coeffs)-0.2, np.abs(Ridge_coefficients)[si], alpha=0.4, label='Ridge')\n",
    "plt.bar(np.arange(n_coeffs)+0.2, np.abs(Lasso_coefficients)[si], alpha=0.2, label='LASSO')\n",
    "plt.bar(np.arange(n_coeffs), -np.abs(OLS_coefficients)[si], color='grey', label='OLS')\n",
    "plt.xticks(range(n_coeffs), feature_names[si], rotation=60)\n",
    "plt.xlim(-1,n_coeffs)\n",
    "plt.ylim(-100,100)\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Let's examine a pair of features we believe to be related.  Is there a difference in the way Ridge and Lasso regression assign coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from HW3 that `atemp` and `temp` are closely correlated.  Looking at the regression coefficients of these two predictors shows how Lasso and Ridge regression performs 'shrinkage' of the coefficient values.  First, examine the coefficient values from the ordinary linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS - temp: 64.2144\n",
      "OLS - atemp: 12.8325\n"
     ]
    }
   ],
   "source": [
    "print(\"OLS - temp: {:.4f}\".format(BikOLSparams['temp']))\n",
    "print(\"OLS - atemp: {:.4f}\".format(BikOLSparams['atemp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, examine the coefficient values of Lasso and Ridge regressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso - temp: 63.8309\n",
      "Lasso - atemp: 12.8684\n",
      "Ridge - temp: 38.4562\n",
      "Ridge - atemp: 30.3841\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso - temp: {:.4f}\".format(BikeLRparams['temp']))\n",
    "print(\"Lasso - atemp: {:.4f}\".format(BikeLRparams['atemp']))\n",
    "\n",
    "print(\"Ridge - temp: {:.4f}\".format(BikeRRparams['temp']))\n",
    "print(\"Ridge - atemp: {:.4f}\".format(BikeRRparams['atemp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that Lasso regression assigns similar coefficient values to the two predictors.  On the other hand, Ridge regression shrinks the value of `temp` such that two coefficient values are closer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.1 How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrinkage penalty) in Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot from Question 2.5, we see noticeable difference in the regression coefficients of plain linear regression and those from Lasso and Ridge regressions.  For instance, in contrast to plain linear regression, Ridge regression places noticeably smaller estimated coefficient values on month predictors (e.g. 'Jul', 'Jun', 'Aug', etc.).  On the other hand, Lasso closely follows the plain linear regression with the exception of 'storm' predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.2 Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>\n",
    "    \n",
    "Interestingly, Ridge regression places the largest absolute coefficient on 'year' predictor and shrinks coefficient values on predictors related to months.  As aforementioned, Lasso closely follows the coefficient estimates of the plain linear regression with exception of 'storm' predictor.\n",
    "\n",
    "Recall from Lab 4 that the penalty function of Ridge regression is\n",
    "$$\\lambda \\sum_{j=1}^m \\beta_{j}^{2}$$\n",
    "while it is \n",
    "$$\\lambda \\sum_{j=1}^m |\\beta_j|$$\n",
    "for Lasso regression.\n",
    "\n",
    "These differences in the penalty function has implications on how the coefficients of Lasso and Ridge regression are estimated, which can be visually explained with geometric interpretation.  \n",
    "\n",
    "![Lasso (L1 Norm) vs Ridge (L2 Norm)](fig/L1_and_L2_balls.svg)\n",
    "\n",
    "Lasso regression can be interpreted as optimization of coefficient values over a cross-polytope (i.e. a diamond in 2-dimenions) and Ridge regression as optimization over a n-sphere (i.e. a circle in 2-dimension).  The differenes in constraint boundaries explain why Lasso and Ridge regression assign different coefficient values to predictors that exhibit high collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.3 Is the significance related to the shrinkage in some way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>\n",
    "    \n",
    "As visible from the geometric interpretation of the penalty function, optimization over a cross-polytope induces sparcity in estimated coefficient values.  As a result, we find that the predictor 'Mon' is not significant in Lasso regression in Question 2.4.  On the other hand, Ridge regression does not induce sparsity in coefficient estimate; consequently, all of the coefficients were significant. \n",
    "\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 3: Polynomial Features, Interaction Terms, and Cross Validation </b> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to fit a model to include all main effects and polynomial terms for numerical predictors up to the $4^{th}$ order. More precisely use the following terms: \n",
    " - predictors in `X_train` and `X_test`\n",
    " - $X^1_j$, $X^2_j$, $X^3_j$, and $X^4_j$ for each numerical predictor $X_j$\n",
    "\n",
    "**3.1** Create an expanded training set including all the desired terms mentioned above. Store that training set (as a pandas dataframe) in the variable `X_train_poly`.  Create the corresponding test set and store it as a pandas dataframe in `X_test_poly`.\n",
    "\n",
    "**3.2** Discuss the following:\n",
    "\n",
    "1. What are the dimensions of this 'design matrix' of all the predictor variables in 3.1?  \n",
    "2. What issues may we run into attempting to fit a regression model using all of these predictors?\n",
    "\n",
    "**3.3** Let's try fitting a regression model on all the predictors anyway.  Use the `LinearRegression` library from `sklearn` to fit a multiple linear regression model to the training set data in `X_train_poly`.  Store the fitted model in the variable `BikeOLSPolyModel`.\n",
    "\n",
    "**3.4** Discuss the following:\n",
    "1. What are the training and test $R^2$ scores? \n",
    "2. How does the model performance compare with the OLS model on the original set of features in Question 1?\n",
    "\n",
    "**3.5** The training set $R^2$ score we generated for our model with polynomial and interaction terms doesn't have any error bars.  Let's use cross-validation to generate sample sets of $R^2$ for our model. Use 5-fold cross-validation to generate $R^2$ scores for the multiple linear regression model with polynomial terms.  What are the mean and standard deviation of the $R^2$ scores for your model.\n",
    "\n",
    "**3.6** Visualize the $R^2$ scores generated from the 5-fold cross validation as a box and whisker plot.\n",
    "\n",
    "**3.7** We've used cross-validation to generate error bars around our $R^2$ scores, but another use of cross-validation is as a way of model selection.  Let's construct the following model alternatives:\n",
    "\n",
    "1. Multiple linear regression model generated based upon the feature set in Question 1 (let's call these the base features.\n",
    "2. base features plus polynomial features to order 2\n",
    "3. base features plus polynomial features to order 4\n",
    "\n",
    "Use 5-fold cross validation on the training set to select the best model.  Make sure to evaluate all the models as much as possible on the same folds.  For each model generate a mean and standard deviation for the $R^2$ score.\n",
    "\n",
    "**3.8** Visualize the $R^2$ scores generated for each model from 5-fold cross validation in box and whiskers plots.  Do the box and whisker plots influence your view of which model was best?\n",
    "\n",
    "**3.9** Evaluate each of the model alternatives on the test set.  How do the results compare with the results from cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create an expanded training set including all the desired terms mentioned above. Store that training set (as a numpy array) in the variable `X_train_poly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Create dataframe of higher order polynomials on a feature column up to order k\n",
    "def gen_df_higher_orders(df, column, k):\n",
    "    \n",
    "    poly_model = PolynomialFeatures(k, include_bias=False)\n",
    "    \n",
    "    feature_data = df[column]\n",
    "    \n",
    "    \n",
    "    # transform to get all the polynomial features of this column\n",
    "    higher_orders = poly_model.fit_transform(feature_data.values.reshape(-1,1))\n",
    "    \n",
    "    feature_names = poly_model.get_feature_names([column])\n",
    "\n",
    "    return pd.DataFrame(higher_orders[:,1:], columns=feature_names[1:])\n",
    "\n",
    "def gen_df_interactions(df, categorical, numerical):\n",
    "    \n",
    "    poly_model= PolynomialFeatures(2, include_bias=False, interaction_only=True)\n",
    "    \n",
    "    all_interactions = pd.DataFrame(poly_model.fit_transform(df), columns = poly_model.get_feature_names(df.columns))\n",
    "    \n",
    "    \n",
    "    # Generate the column names from interacations between categorical columns\n",
    "    # including the categorical columns themselves and drop them\n",
    "    poly_model.fit_transform(X_train[categorical])\n",
    "\n",
    "    all_interactions = all_interactions.drop(poly_model.get_feature_names(categorical), axis=1)\n",
    "    \n",
    "    \n",
    "    # Generate the column names from interactions between numerical columns\n",
    "    # including the numerical columns themselves and drop them\n",
    "    poly_model.fit_transform(X_train[numerical])\n",
    "\n",
    "    all_interactions = all_interactions.drop(poly_model.get_feature_names(numerical), axis=1)\n",
    "    \n",
    "    return all_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poly_dataset(dataset_train, dataset_test, k):\n",
    "    \n",
    "    higher_orders_train =[gen_df_higher_orders(dataset_train, feature, k) for feature in numerical_columns]\n",
    "    higher_orders_test = [gen_df_higher_orders(dataset_test, feature, k) for feature in numerical_columns]\n",
    "\n",
    "    higher_orders_train = pd.concat(higher_orders_train, axis=1)\n",
    "    higher_orders_test = pd.concat(higher_orders_test, axis=1)\n",
    "\n",
    "    higher_orders_columns = higher_orders_train.columns\n",
    "\n",
    "    # scale higher order polynomial features\n",
    "    scaler = StandardScaler().fit(higher_orders_train)\n",
    "\n",
    "    higher_orders_train[higher_orders_columns] = scaler.transform(higher_orders_train)\n",
    "    higher_orders_test[higher_orders_columns] = scaler.transform(higher_orders_test)\n",
    "\n",
    "    # concatenate to get the full data frames\n",
    "    poly_train = pd.concat([dataset_train.reset_index().drop(['index'], axis=1)] + [higher_orders_train], axis=1)\n",
    "    poly_test = pd.concat([dataset_test.reset_index().drop(['index'], axis=1)] + [higher_orders_test], axis=1)\n",
    "    \n",
    "    return (poly_train, poly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_train, X_poly_test = get_poly_dataset(X_train, X_test, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 What are the dimensions of this 'design matrix'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hour', 'holiday', 'year', 'workingday', 'temp', 'atemp', 'hum',\n",
       "       'windspeed', 'spring', 'summer', 'fall', 'Feb', 'Mar', 'Apr', 'May',\n",
       "       'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec', 'Mon', 'Tue', 'Wed',\n",
       "       'Thu', 'Fri', 'Sat', 'Cloudy', 'Snow', 'Storm', 'temp^2', 'temp^3',\n",
       "       'temp^4', 'atemp^2', 'atemp^3', 'atemp^4', 'hum^2', 'hum^3', 'hum^4',\n",
       "       'windspeed^2', 'windspeed^3', 'windspeed^4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>holiday</th>\n",
       "      <th>year</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>spring</th>\n",
       "      <th>summer</th>\n",
       "      <th>...</th>\n",
       "      <th>temp^4</th>\n",
       "      <th>atemp^2</th>\n",
       "      <th>atemp^3</th>\n",
       "      <th>atemp^4</th>\n",
       "      <th>hum^2</th>\n",
       "      <th>hum^3</th>\n",
       "      <th>hum^4</th>\n",
       "      <th>windspeed^2</th>\n",
       "      <th>windspeed^3</th>\n",
       "      <th>windspeed^4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>13903.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "      <td>1.390300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.529454</td>\n",
       "      <td>0.029274</td>\n",
       "      <td>0.504567</td>\n",
       "      <td>0.680644</td>\n",
       "      <td>4.533971e-15</td>\n",
       "      <td>-5.184182e-16</td>\n",
       "      <td>-6.715081e-16</td>\n",
       "      <td>2.018637e-15</td>\n",
       "      <td>0.253183</td>\n",
       "      <td>0.258865</td>\n",
       "      <td>...</td>\n",
       "      <td>2.799285e-15</td>\n",
       "      <td>5.740771e-16</td>\n",
       "      <td>8.715846e-16</td>\n",
       "      <td>4.254602e-15</td>\n",
       "      <td>1.907638e-15</td>\n",
       "      <td>-1.058247e-16</td>\n",
       "      <td>-3.434672e-15</td>\n",
       "      <td>9.098910e-16</td>\n",
       "      <td>6.119982e-16</td>\n",
       "      <td>-3.260716e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.917884</td>\n",
       "      <td>0.168580</td>\n",
       "      <td>0.499997</td>\n",
       "      <td>0.466244</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>0.434850</td>\n",
       "      <td>0.438027</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.474055e+00</td>\n",
       "      <td>-2.764988e+00</td>\n",
       "      <td>-3.257769e+00</td>\n",
       "      <td>-1.557515e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.147625e-01</td>\n",
       "      <td>-9.304302e-01</td>\n",
       "      <td>-8.220754e+00</td>\n",
       "      <td>-4.712201e-01</td>\n",
       "      <td>-9.190551e-01</td>\n",
       "      <td>-1.281820e+01</td>\n",
       "      <td>-3.909072e-01</td>\n",
       "      <td>-6.197134e-01</td>\n",
       "      <td>-8.102365e-01</td>\n",
       "      <td>-1.785218e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.133773e-01</td>\n",
       "      <td>-8.265766e-01</td>\n",
       "      <td>-7.719159e-01</td>\n",
       "      <td>-7.009933e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.060281e-01</td>\n",
       "      <td>-7.928673e-01</td>\n",
       "      <td>-1.881868e-01</td>\n",
       "      <td>-4.663738e-01</td>\n",
       "      <td>-7.650356e-01</td>\n",
       "      <td>-1.273693e-01</td>\n",
       "      <td>-3.858789e-01</td>\n",
       "      <td>-5.725461e-01</td>\n",
       "      <td>-1.709233e-01</td>\n",
       "      <td>-1.782270e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.696154e-02</td>\n",
       "      <td>5.451955e-02</td>\n",
       "      <td>4.913269e-03</td>\n",
       "      <td>3.258315e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.051289e-01</td>\n",
       "      <td>-2.619651e-01</td>\n",
       "      <td>3.236408e-02</td>\n",
       "      <td>-3.581280e-01</td>\n",
       "      <td>-3.574097e-01</td>\n",
       "      <td>4.373286e-02</td>\n",
       "      <td>-3.240565e-01</td>\n",
       "      <td>-3.155264e-01</td>\n",
       "      <td>-1.067849e-01</td>\n",
       "      <td>-1.665422e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.473004e-01</td>\n",
       "      <td>8.477969e-01</td>\n",
       "      <td>7.817424e-01</td>\n",
       "      <td>5.219073e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.585169e-02</td>\n",
       "      <td>4.103700e-01</td>\n",
       "      <td>2.702095e-01</td>\n",
       "      <td>-1.693190e-02</td>\n",
       "      <td>5.120220e-01</td>\n",
       "      <td>2.214529e-01</td>\n",
       "      <td>4.308982e-02</td>\n",
       "      <td>1.776485e-01</td>\n",
       "      <td>-8.032379e-02</td>\n",
       "      <td>-9.642754e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.611770e+00</td>\n",
       "      <td>3.050828e+00</td>\n",
       "      <td>1.921092e+00</td>\n",
       "      <td>5.415149e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114033e+01</td>\n",
       "      <td>7.738178e+00</td>\n",
       "      <td>1.111864e+01</td>\n",
       "      <td>1.849309e+01</td>\n",
       "      <td>8.835145e+00</td>\n",
       "      <td>2.681217e+00</td>\n",
       "      <td>1.977104e+01</td>\n",
       "      <td>1.757132e+01</td>\n",
       "      <td>2.945729e+01</td>\n",
       "      <td>4.248249e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               hour       holiday          year    workingday          temp  \\\n",
       "count  13903.000000  13903.000000  13903.000000  13903.000000  1.390300e+04   \n",
       "mean      11.529454      0.029274      0.504567      0.680644  4.533971e-15   \n",
       "std        6.917884      0.168580      0.499997      0.466244  1.000036e+00   \n",
       "min        0.000000      0.000000      0.000000      0.000000 -2.474055e+00   \n",
       "25%        6.000000      0.000000      0.000000      0.000000 -8.133773e-01   \n",
       "50%       11.000000      0.000000      1.000000      1.000000  1.696154e-02   \n",
       "75%       18.000000      0.000000      1.000000      1.000000  8.473004e-01   \n",
       "max       23.000000      1.000000      1.000000      1.000000  2.611770e+00   \n",
       "\n",
       "              atemp           hum     windspeed        spring        summer  \\\n",
       "count  1.390300e+04  1.390300e+04  1.390300e+04  13903.000000  13903.000000   \n",
       "mean  -5.184182e-16 -6.715081e-16  2.018637e-15      0.253183      0.258865   \n",
       "std    1.000036e+00  1.000036e+00  1.000036e+00      0.434850      0.438027   \n",
       "min   -2.764988e+00 -3.257769e+00 -1.557515e+00      0.000000      0.000000   \n",
       "25%   -8.265766e-01 -7.719159e-01 -7.009933e-01      0.000000      0.000000   \n",
       "50%    5.451955e-02  4.913269e-03  3.258315e-02      0.000000      0.000000   \n",
       "75%    8.477969e-01  7.817424e-01  5.219073e-01      1.000000      1.000000   \n",
       "max    3.050828e+00  1.921092e+00  5.415149e+00      1.000000      1.000000   \n",
       "\n",
       "           ...             temp^4       atemp^2       atemp^3       atemp^4  \\\n",
       "count      ...       1.390300e+04  1.390300e+04  1.390300e+04  1.390300e+04   \n",
       "mean       ...       2.799285e-15  5.740771e-16  8.715846e-16  4.254602e-15   \n",
       "std        ...       1.000036e+00  1.000036e+00  1.000036e+00  1.000036e+00   \n",
       "min        ...      -5.147625e-01 -9.304302e-01 -8.220754e+00 -4.712201e-01   \n",
       "25%        ...      -5.060281e-01 -7.928673e-01 -1.881868e-01 -4.663738e-01   \n",
       "50%        ...      -4.051289e-01 -2.619651e-01  3.236408e-02 -3.581280e-01   \n",
       "75%        ...       5.585169e-02  4.103700e-01  2.702095e-01 -1.693190e-02   \n",
       "max        ...       1.114033e+01  7.738178e+00  1.111864e+01  1.849309e+01   \n",
       "\n",
       "              hum^2         hum^3         hum^4   windspeed^2   windspeed^3  \\\n",
       "count  1.390300e+04  1.390300e+04  1.390300e+04  1.390300e+04  1.390300e+04   \n",
       "mean   1.907638e-15 -1.058247e-16 -3.434672e-15  9.098910e-16  6.119982e-16   \n",
       "std    1.000036e+00  1.000036e+00  1.000036e+00  1.000036e+00  1.000036e+00   \n",
       "min   -9.190551e-01 -1.281820e+01 -3.909072e-01 -6.197134e-01 -8.102365e-01   \n",
       "25%   -7.650356e-01 -1.273693e-01 -3.858789e-01 -5.725461e-01 -1.709233e-01   \n",
       "50%   -3.574097e-01  4.373286e-02 -3.240565e-01 -3.155264e-01 -1.067849e-01   \n",
       "75%    5.120220e-01  2.214529e-01  4.308982e-02  1.776485e-01 -8.032379e-02   \n",
       "max    8.835145e+00  2.681217e+00  1.977104e+01  1.757132e+01  2.945729e+01   \n",
       "\n",
       "        windspeed^4  \n",
       "count  1.390300e+04  \n",
       "mean  -3.260716e-16  \n",
       "std    1.000036e+00  \n",
       "min   -1.785218e-01  \n",
       "25%   -1.782270e-01  \n",
       "50%   -1.665422e-01  \n",
       "75%   -9.642754e-02  \n",
       "max    4.248249e+01  \n",
       "\n",
       "[8 rows x 43 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "13903 x 43\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 What issues may we run into attempting to fit a regression model using all of these predictors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Overfitting may be a problem when applying a trained regression models using polynomial features.\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Let's try fitting a regression model on all the predictors anyway.  Use the `LinearRegression` library from `sklearn` to fit a multiple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "BikeOLSPolyModel = LinearRegression()\n",
    "\n",
    "BikeOLSPolyModel.fit(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 What are the training and test $R^2$ scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': 0.42230805166587104, 'test': 0.42027912762252395}\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "BikeOLSPoly_r2scores = {}\n",
    "\n",
    "BikeOLSPoly_r2scores['training'] = BikeOLSPolyModel.score(X_poly_train, y_train)\n",
    "BikeOLSPoly_r2scores['test'] = BikeOLSPolyModel.score(X_poly_test, y_test)\n",
    "\n",
    "print(BikeOLSPoly_r2scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Training: 0.422\n",
    "    \n",
    "Test: 0.420\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 How does the model performance compare with the OLS model on the original set of features in Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the original linear model from question 1 is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': 0.4065387827969087, 'test': 0.40638554757102285}\n"
     ]
    }
   ],
   "source": [
    "print(BikeOLS_r2scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the polynomial regression models performs slightly better in terms of $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 The training set $R^2$ score we generated for our model with polynomial and interaction terms doesn't have any error bars.  Let's use cross-validation to generate sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.26727805322296977\n",
      "Std. Dev:  0.09109366555046658\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "    \n",
    "MLR_scores = cross_val_score(LinearRegression(), X_poly_train, y_train, cv=5)\n",
    "    \n",
    "print(\"Mean: \", np.mean(MLR_scores))\n",
    "print(\"Std. Dev: \", np.std(MLR_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got error bars for our training set $R^2$.  These scores aren't comparable to those we got in 3.4 because they're training set and not validation set scores, but we at least have a sense for the uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Plot the $R^2$ scores generated from k-fold cross validation (for each k) as a box and whisker plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.boxplot(MLR_scores)\n",
    "plt.ylabel(r\"$R^2$\", fontsize=18)\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 We've used cross-validation to generate error bars around our $R^2$ scores, but another use of cross-validation is as a way of model selection.  Let's construct the following model alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "kfold_ks = list(range(5))\n",
    "\n",
    "x_kfold_scores = []\n",
    "poly2_kfold_scores = []\n",
    "poly4_kfold_scores = []\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(k_fold.split(X_train, y_train)):\n",
    "    # Create k-th Train & Test DataFrames\n",
    "    X_kfold_train = pd.DataFrame(X_train.values[train_index], columns=X_train.columns)  \n",
    "    X_kfold_test =  pd.DataFrame(X_train.values[test_index], columns=X_train.columns)\n",
    "    \n",
    "    y_kfold_train, y_kfold_test = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    # Create 2-nd and 4-th degree polynomial feature design matrices\n",
    "    poly2_kfold_train, poly2_kfold_test = get_poly_dataset(X_kfold_train, X_kfold_test, 2)\n",
    "    poly4_kfold_train, poly4_kfold_test = get_poly_dataset(X_kfold_train, X_kfold_test, 4)\n",
    "    \n",
    "    # Train\n",
    "    kfold = LinearRegression().fit(X_kfold_train, y_kfold_train)\n",
    "    poly2 = LinearRegression().fit(poly2_kfold_train, y_kfold_train)\n",
    "    poly4 = LinearRegression().fit(poly4_kfold_train, y_kfold_train)\n",
    "    \n",
    "    # Score\n",
    "    x_kfold_scores.append(kfold.score(X_kfold_test, y_kfold_test))\n",
    "    poly2_kfold_scores.append(poly2.score(poly2_kfold_test, y_kfold_test))\n",
    "    poly4_kfold_scores.append(poly4.score(poly4_kfold_test, y_kfold_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kfold_scores = np.array([x_kfold_scores, poly2_kfold_scores, poly4_kfold_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: Base Features\n",
      "Mean:  0.2579944364362188\n",
      "Std. Dev: 0.0922005509622767\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 1: Base Features\")\n",
    "print(\"Mean: \", all_kfold_scores.mean(axis=1)[0])\n",
    "print(\"Std. Dev:\", all_kfold_scores.std(axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2:  Base Features plus polynomial features to order 2\n",
      "Mean:  0.26325875116942654\n",
      "Std. Dev: 0.0767035248374377\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 2:  Base Features plus polynomial features to order 2\")\n",
    "print(\"Mean: \", np.mean(poly2_kfold_scores))\n",
    "print(\"Std. Dev:\", np.std(poly2_kfold_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3:  Base Features plus polynomial features to order 4\n",
      "Mean:  0.2672780532229682\n",
      "Std. Dev: 0.0910936655504684\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 3:  Base Features plus polynomial features to order 4\")\n",
    "print(\"Mean: \", np.mean(poly4_kfold_scores))\n",
    "print(\"Std. Dev:\", np.std(poly4_kfold_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based purely on model selection via k-fold cross validation, the base model with polynomial features of degree 4 is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Visualize the $R^2$ scores generated for each model from 5-fold cross validation in box and whiskers plots.  Do the box and whisker plots influence your view of which model was best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.boxplot([x_kfold_scores, poly2_kfold_scores, poly4_kfold_scores])\n",
    "plt.ylabel(r\"$R^2$\", fontsize=18)\n",
    "plt.xticks(range(1,4), ['Linear (Base)', '2 Order Polynomial', '4 Order Polynomial'], fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 Evaluate each of the model alternatives on the  test set.  How do the results compare with the results from cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2_train, poly2_test = get_poly_dataset(X_train, X_test, 2)\n",
    "poly4_train, poly4_test = get_poly_dataset(X_train, X_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LinearRegression().fit(X_train, y_train)\n",
    "poly2_model = LinearRegression().fit(poly2_train, y_train)\n",
    "poly4_model = LinearRegression().fit(poly4_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for Base Model on Test:  0.40638554757102285\n",
      "R-squared for 2 Degree Polynomial Features Model on Test:  0.41076037422804523\n",
      "R-squared for 4 Degree Polynomial Features Model on Test:  0.42027912762252395\n"
     ]
    }
   ],
   "source": [
    "print(\"R-squared for Base Model on Test: \", base_model.score(X_test, y_test))\n",
    "print(\"R-squared for 2 Degree Polynomial Features Model on Test: \", poly2_model.score(poly2_test, y_test))\n",
    "print(\"R-squared for 4 Degree Polynomial Features Model on Test: \", poly4_model.score(poly4_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models performed better on the test set than on the CV validation sets.  However, the relative model performance was somewhat similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
